### File - parse_results.py

import csv
from statistics import *

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold
from sklearn.tree import DecisionTreeClassifier


def find_k_best(fname, k=20):
    """
    This function takes in a file with results generated by the main.py module. After running main.py, results are
    parsed in this function. This function generates tables for how well what features did, on average and by each
    classifier. In the future, some deeper analytics will be included, like statistics.

    :param fname: file containing results generated by main.py
    :param k: the number of results to record.
    :return: None, all operations are written to disk as a csv file.
    """

    # what features were used to generate each classifier.
    features = []
    res_avg = []

    # these arrays will keep track of the numerical values of the AUC scores for each classifier
    res_dt = []
    res_rfw = []

    # collection of results for each classifier
    res = [res_dt, res_rfw]
    f = open(fname)
    for i, line in enumerate(f):
        # each of the variables extracts the respective value from each line of the tsv input file.
        # rdt is the accuracy for decision tree and so on
        rdt, rrfw, feature = line.split('\t')
        res_dt.append((float(rdt), i))
        res_rfw.append((float(rrfw), i))

        # we calculate the average of each classifier on each occasion.
        res_avg.append((sum([x[i][0] for x in res]) / len(res), i))

        # finally track what features were used to generate this flow.
        features.append(feature.split(','))

    res_avg = sorted(res_avg, reverse=True)
    res_dt = sorted(res_dt, reverse=True)
    res_rfw = sorted(res_rfw, reverse=True)

    # fina CA's for the best scores

    ca_dtree = [find_ca('dt', features[x[1]]) for x in res_dt[:min(k, len(res_avg))]]
    ca_rfw = [find_ca('rfw', features[x[1]]) for x in res_rfw[:min(k, len(res_avg))]]
    ca_avg = [np.mean(x) for x in zip(ca_dtree, ca_rfw)]

    arr = [['Average AUC', 'Average CA', 'Average AUC Features', 'DT AUC', 'DT CA', 'DT AUC Features', 'RFW AUC',
            'RFW CA', 'RFW AUC Features']]

    # generate array with column header specified above. This array will be written to a csv file.
    # arr is a matrix of dimension (k + 1) * 10 columns.
    # print(features[res_avg[0][1]])
    for i in range(min(k, len(res_avg))):
        arr.append([
            res_avg[i][0], ca_avg[i], ', '.join([x.strip() for x in features[res_avg[i][1]]]),
            res_dt[i][0], ca_dtree[i], ', '.join([x.strip() for x in features[res_dt[i][1]]]),
            res_rfw[i][0], ca_rfw[i], ', '.join([x.strip() for x in features[res_rfw[i][1]]]),

            ])
    
    print('finished writing to', fname)
    # write to csv file
    f = open(fname + 'parsed.csv', 'w', newline='')
    writer = csv.writer(f, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    writer.writerows(arr)


def find_ca(clf, features):
    """
    Find classification accuracy for a given kind of classifier.
    :param clf: either rfw or dt
    :param features: features to try
    :return: classification accuracy,  single number
    """
    ''' HYPERPARAMS FOR DECISION TREE

     These parameters implement a rudimentary pruning algorithm, would ideally like to use AB pruning'''
    enable_pruning = True
    # maximum depth of dtree
    max_depth = 5
    # how many samples your need atleast, at a LEAF node
    min_samples = 3

    d_trees = []

    all_data, all_labels, test_data, test_labels, train_data, train_labels = load_data()

    aucs = []
    # make fold
    skf = StratifiedKFold(n_splits=10, shuffle=True)
    for trx, tex in skf.split(all_data, all_labels):
        # strip data to required features
        subset_data = all_data.filter(features, axis=1)

        if clf == 'rfw':
            # find auc
            rfwtree = RandomForestClassifier(n_estimators=100)
            rfwtree.fit(subset_data.iloc[trx, :], all_labels.iloc[trx])
            pred = rfwtree.predict(subset_data.iloc[tex, :])
            labels = all_labels.iloc[tex]

            acc = roc_auc_score(labels, pred)
            # record auc to average later
            aucs.append(acc)
        else:
            # find auc
            dtree = DecisionTreeClassifier(presort=True, max_depth=max_depth, min_samples_leaf=min_samples)
            dtree.fit(subset_data.iloc[trx, :], all_labels.iloc[trx])
            pred = dtree.predict(subset_data.iloc[tex, :])
            labels = all_labels.iloc[tex]

            acc = roc_auc_score(labels, pred)
            # record auc to average later
            aucs.append(acc)

    return np.mean(aucs)


def load_data():
    """
    Loads in data from given parameters.
    :return: all_data, all_labels, test_data, test_labels, train_data, train_labels
    """
    path_train_data = 'train.csv'
    path_test_data = 'test.csv'
    path_all_data = 'Dataset Correlated Removed.csv'

    # load dataset
    all_data = pd.DataFrame(pd.read_csv(path_all_data))
    all_labels = all_data['SLC'].astype('category').cat.codes
    # drop labels
    all_data.drop('SLC', axis=1, inplace=True)

    train_data = pd.DataFrame(pd.read_csv(path_train_data))
    train_labels = train_data['SLC'].astype('category').cat.codes
    # drop labels

    train_data.drop('SLC', axis=1, inplace=True)

    test_data = pd.DataFrame(pd.read_csv(path_test_data))
    test_labels = test_data['SLC'].astype('category').cat.codes
    # drop labels
    test_data.drop('SLC', axis=1, inplace=True)

    return all_data, all_labels, test_data, test_labels, train_data, train_labels


def find_best_over(k=100, f=[2, 5, 6, 7], fnameprefix='results'):
    """
    Searches for results files for each value listed in f, in the present directory and attempts to parse each file.
    It does so by calling the @find_k_best method with parameter k, for every value in f. fnameprefix is the prefix
    given to the results file, which is 'results<num features>' by default, so results6 for 6 features and so forth.

    :param k: number of top results to filter.
    :param f: a list containing the number of features tried.
    :param fnameprefix: the prefix of the name of the results files, see docstring of this method for clarification.
    :return: None, writes everything to disk.
    """

    for val in f:
        # try:
        find_k_best(fnameprefix + str(val), k)
        # except:
        #   print('Tried searching for file named', fnameprefix + str(val), 'unsuccessfully. This file was skipped.')


#

## Run the script over here
# find_best_over(300, [2, 3, 4, 5, 6, 7])
find_best_over(2500, [2, 3, 4, 5, 6, 7, 8])
#find_best_over(300, [2, 3, 4, 5, 6])


# find_k_best('results2', 40)
## These functions will be defined at some  point in the future, do not contain anything right now.
def find_stats(arr):
    pass
    return mean, std, var


def commonfeatures_k_best(fname):
    pass
